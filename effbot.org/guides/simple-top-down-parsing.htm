<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html>
<head> <meta http-equiv="content-type" content="text/html; charset=utf-8"> <link rel="alternate" type="application/rss+xml" title="RSS" href="rss.xml"> <link rel="shortcut icon" href="/media/img/effbot.ico"> <link rel="stylesheet" href="/media/css/effbot-min.css" type="text/css" media="screen"> <link rel="stylesheet" href="/media/css/effbotprint-min.css" type="text/css" media="print"> <title>Simple Top-Down Parsing in&nbsp;Python</title> <script type="text/javascript">effbot_page_id=2021;</script> </head> <body> <div id="doc2" class="yui-t2"> <div id="hd"> <!-- header --> <!--
<p style='color: #8f8f8f; background: #fff5bf; padding: 5px 10px;'> <b>2008-07-15:</b> Selected articles
 (including this one) 
now have experimental "comment" links in the left column and at the bottom.
You're welcome to use them for commenting and voting on articles.
For a bit more on this, see <a
href="http://effbot.slinkset.com/links/Welcome_to_discuss_effbot_org">this
page</a>. /F
</p>
--> </div> <!-- hd --> <div id="bd"> <!-- body --> <div id="yui-main"> <div class="yui-b"> <div class="content"><div class="yui-g"> <h1 class="maintitle">Simple Top-Down Parsing in&nbsp;Python</h1> </div> <!-- yui-g --><div class="yui-ge"><div class="yui-u first"><p class="info">Fredrik Lundh | July 2008</p><p>In <a href="http://effbot.org/zone/simple-iterator-parser.htm"><em>Simple Iterator-based Parsing</em></a>, I described a way to write simple recursive-descent parsers in Python, by passing around the current token and a token generator function.</p><p>A recursive-descent parser consists of a series of functions, usually one for each grammar rule.  Such parsers are easy to write, and are reasonably efficient, as long as the grammar is &#8220;prefix-heavy&#8221;; that is, that it&#8217;s usually sufficient to look at a token at the beginning of a construct to figure out what parser function to call.  For example, if you&#8217;re parsing Python code, you can identify most statements simply by looking at the first token.</p><p>However, recursive-descent is less efficient for expression syntaxes, especially for languages with lots of operators at different precedence levels.  With one function per rule, you can easily end up with lots of calls even for short, trivial expressions, just to get to the right level in the grammar.</p><p>For example, here&#8217;s an excerpt from Python&#8217;s expression grammar.  The &#8220;test&#8221; rule is a basic expression element:</p><pre><code>test: or_test ['if' or_test 'else' test] | lambdef
or_test: and_test ('or' and_test)*
and_test: not_test ('and' not_test)*
not_test: 'not' not_test | comparison
comparison: expr (comp_op expr)*
expr: xor_expr ('|' xor_expr)*
xor_expr: and_expr ('^' and_expr)*
and_expr: shift_expr ('&amp;' shift_expr)*
shift_expr: arith_expr (('&lt;&lt;'|'&gt;&gt;') arith_expr)*
arith_expr: term (('+'|'-') term)*
term: factor (('*'|'/'|'%'|'//') factor)*
factor: ('+'|'-'|'~') factor | power
power: atom trailer* ['**' factor]
trailer: '(' [arglist] ')' | '[' subscriptlist ']' | '.' NAME
</code></pre><p>With a naive recursive-descent implementation of this grammar, 
the parser would have to recurse all the way from &#8220;test&#8221; down to &#8220;trailer&#8221; in order to parse a simple function call (of the form &#8220;expression(arglist)&#8221;).</p><p>In the early seventies, Vaughan Pratt published an elegant improvement to recursive-descent in his paper <a href="http://doi.acm.org/10.1145/512927.512931"><em>Top-down Operator Precedence</em></a>.  Pratt&#8217;s algorithm associates semantics with tokens instead of grammar rules, and uses a simple &#8220;binding power&#8221; mechanism to handle precedence levels.  Traditional recursive-descent parsing is then used to handle odd or irregular portions of the syntax.</p><p>In an <a href="http://javascript.crockford.com/tdop/tdop.html">article</a> (and <a href="http://oreilly.com/catalog/9780596510046/">book chapter</a>) with the same name, Douglas Crockford shows how to implement the algorithm in a subset of JavaScript, and uses it to develop a parser that can parse itself in the process.</p><p>In this article, I&#8217;ll be a bit more modest: I&#8217;ll briefly explain how the algorithm works, discuss different ways to implement interpreters and translators with it in Python, and finally use it to implement a parser for Python&#8217;s expression syntax.  And yes, there will be benchmarks too.</p><h2 id="introducing-the-algorithm">Introducing The Algorithm&#160;<a class="nav" href="#introducing-the-algorithm" title="bookmark!">#</a></h2><p>Like most other parsers, a topdown parser operates on a stream of distinct syntax elements, or tokens.  For example, the expression &#8220;1 + 2&#8221; could correspond to the following tokens:</p><pre><code>literal with value 1
add operator
literal with value 2
end of program
</code></pre><p>In the topdown algorithm, each token has two associated functions, called &#8220;nud&#8221; and &#8220;led&#8221;, and an integer value called &#8220;lbp&#8221;.</p><p>The &#8220;nud&#8221; function (for null denotation) is used when a token appears at the beginning of a language construct, and the &#8220;led&#8221; function (left denotation) when it appears inside the construct (to the left of the rest of the construct, that is).</p><p>The &#8220;lbp&#8221; value is a binding power, and it controls operator precedence; the higher the value, the tighter a token binds to the tokens that follow.</p><p>Given this brief introduction, we&#8217;re ready to look at the core of Pratt&#8217;s algorithm, the expression parser:</p><pre class="python"><span class="pykeyword">def</span> <span class="pyfunction">expression</span>(rbp=0):
    <span class="pykeyword">global</span> token
    t = token
    token = next()
    left = t.nud()
    <span class="pykeyword">while</span> rbp &lt; token.lbp:
        t = token
        token = next()
        left = t.led(left)
    <span class="pykeyword">return</span> left</pre><p>(Pratt calls this function &#8220;parse&#8221;, but we&#8217;ll use the name from Crockford&#8217;s article instead.)</p><p>Here, &#8220;token&#8221; is a global variable that contains the current token, and &#8220;next&#8221; is a global helper that fetches the next token.  The &#8220;nud&#8221; and &#8220;led&#8221; functions are represented as methods, and the &#8220;lbp&#8221; is an attribute.  The &#8220;left&#8221; variable, finally, is used to pass some value that represents the &#8220;left&#8221; part of the expression through to the &#8220;led&#8221; method; this can be any object, such as an intermediate result (for an interpreter) or a portion of a parse tree (for a compiler).</p><p>If applied to the simple expression shown earlier, the parser will start by calling the &#8220;nud&#8221; method on the first token.  In our example, that&#8217;s a literal token, which can be represented by something like the following class:</p><pre class="python"><span class="pykeyword">class</span> <span class="pyclass">literal_token</span>:
    <span class="pykeyword">def</span> <span class="pyfunction">__init__</span>(self, value):
        self.value = int(value)
    <span class="pykeyword">def</span> <span class="pyfunction">nud</span>(self):
        <span class="pykeyword">return</span> self.value</pre><p>Next, the parser checks if the binding power of the next token is at least as large as the given binding power (the &#8220;rbp&#8221; argument, for &#8220;right binding power&#8221;).  If it is, it calls the &#8220;led&#8221; method for that token.  Here, the right binding power is zero, and the next token is an operator, the implementation of which could look like:</p><pre class="python"><span class="pykeyword">class</span> <span class="pyclass">operator_add_token</span>:
    lbp = 10
    <span class="pykeyword">def</span> <span class="pyfunction">led</span>(self, left):
        right = expression(10)
        <span class="pykeyword">return</span> left + right</pre><p>The operator has a binding power of 10, and a &#8220;led&#8221; method that calls the expression parser again, passing in a right binding power that&#8217;s the same as the operator&#8217;s own power.  This causes the expression parser to treat everything with a higher power as a subexpression, and return its result.  The method then adds the left value (from the literal, in this case) to the return value from the expression parser, and returns the result.</p><p>The end of the program is indicated by a special marker token, with binding power zero (lower than any other token).  This makes sure that the expression parser stops when it reaches the end of the program.</p><pre class="python"><span class="pykeyword">class</span> <span class="pyclass">end_token</span>:
    lbp = 0</pre><p>And that&#8217;s the entire parser.  To use it, we need a tokenizer that can generate the right kind of token objects for a given source program.  Here&#8217;s a simple regular expression-based version that handles the minimal language we&#8217;ve used this far:</p><pre class="python"><span class="pykeyword">import</span> re

token_pat = re.compile(<span class="pystring">"\s*(?:(\d+)|(.))"</span>)

<span class="pykeyword">def</span> <span class="pyfunction">tokenize</span>(program):
    <span class="pykeyword">for</span> number, operator <span class="pykeyword">in</span> token_pat.findall(program):
        <span class="pykeyword">if</span> number:
            <span class="pykeyword">yield</span> literal_token(number)
        <span class="pykeyword">elif</span> operator == <span class="pystring">"+"</span>:
            <span class="pykeyword">yield</span> operator_add_token()
        <span class="pykeyword">else</span>:
            <span class="pykeyword">raise</span> SyntaxError(<span class="pystring">"unknown operator"</span>)
    <span class="pykeyword">yield</span> end_token()</pre><p>Now, let&#8217;s wire this up and try it out:</p><pre class="python"><span class="pykeyword">def</span> <span class="pyfunction">parse</span>(program):
    <span class="pykeyword">global</span> token, next
    next = tokenize(program).next
    token = next()
    <span class="pykeyword">return</span> expression()

&gt;&gt;&gt; parse(<span class="pystring">"1 + 2"</span>)
3</pre><p>Not counting the calls to the tokenizer, the parser algorithm will make a total of four calls to parse this expression; one for each token, and one extra for the recursive call to the expression parser in the &#8220;led&#8221; method.</p><h3 id="extending-the-parser">Extending the Parser&#160;<a class="nav" href="#extending-the-parser" title="bookmark!">#</a></h3><p>To see how this scales, let&#8217;s add support for a few more math operations.  We need a few more token classes:</p><pre class="python"><span class="pykeyword">class</span> <span class="pyclass">operator_sub_token</span>:
    lbp = 10
    <span class="pykeyword">def</span> <span class="pyfunction">led</span>(self, left):
        <span class="pykeyword">return</span> left - expression(10)

<span class="pykeyword">class</span> <span class="pyclass">operator_mul_token</span>:
    lbp = 20
    <span class="pykeyword">def</span> <span class="pyfunction">led</span>(self, left):
        <span class="pykeyword">return</span> left * expression(20)

<span class="pykeyword">class</span> <span class="pyclass">operator_div_token</span>:
    lbp = 20
    <span class="pykeyword">def</span> <span class="pyfunction">led</span>(self, left):
        <span class="pykeyword">return</span> left / expression(20)</pre><p>Note that &#8220;mul&#8221; and &#8220;div&#8221; uses a higher binding power than the other operators; this guarantees that when the &#8220;mul&#8221; operator is invoked in the expression &#8220;1 * 2 + 3&#8221;, it only gets the literal &#8220;2&#8221;, instead of treating &#8220;2 + 3&#8221; as a subexpression.</p><p>We also need to add the classes to the tokenizer:</p><pre><code>def tokenize(program):
    for number, operator in token_pat.findall(program):
        if number:
            yield literal_token(number)
        elif operator == "+":
            yield operator_add_token()
        elif operator == "-":
            yield operator_sub_token()
        elif operator == "*":
            yield operator_mul_token()
        elif operator == "/":
            yield operator_div_token()
        else:
            raise SyntaxError("unknown operator)
    yield end_token()
</code></pre><p>but that&#8217;s it.  The parser now understands the four basic math operators, and handles their precedence correctly.</p><pre class="python">&gt;&gt;&gt; parse(<span class="pystring">"1+2"</span>)
3
&gt;&gt;&gt; parse(<span class="pystring">"1+2*3"</span>)
7
&gt;&gt;&gt; parse(<span class="pystring">"1+2-3*4/5"</span>)
1</pre><p>Despite the fact that we&#8217;ve added more grammar rules, the parser still makes the same number of calls as before; the expression &#8220;1+2&#8221; is still handled by four calls inside the parser.</p><p>However, codewise, this isn&#8217;t that different from a recursive-descent parser.  We still need to write code for each token class, and while we&#8217;ve moved most of the dispatching from individual rules to the expression parser, most of that ended up in a big if/else statement in the tokenizer.</p><p>Before we look at ways to get rid of some of that code, let&#8217;s add two more features to the parser: unary plus and minus operators, and a Python-style exponentiation operator (**).</p><p>To support the unary operators, all we need to do is to add &#8220;nud&#8221; implementations to the relevant tokens:</p><pre class="python"><span class="pykeyword">class</span> <span class="pyclass">operator_add_token</span>:
    lbp = 10
    <span class="pykeyword">def</span> <span class="pyfunction">nud</span>(self):
        <span class="pykeyword">return</span> expression(100)
    <span class="pykeyword">def</span> <span class="pyfunction">led</span>(self, left):
        <span class="pykeyword">return</span> left + expression(10)

<span class="pykeyword">class</span> <span class="pyclass">operator_sub_token</span>:
    lbp = 10
    <span class="pykeyword">def</span> <span class="pyfunction">nud</span>(self):
        <span class="pykeyword">return</span> -expression(100)
    <span class="pykeyword">def</span> <span class="pyfunction">led</span>(self, left):
        <span class="pykeyword">return</span> left - expression(10)</pre><p>Note that the recursive call to expression uses a high binding power, to make sure that the unary operator binds to the token immediately to the right, instead of to the rest of the expression (&#8220;(-1)-2&#8221; and &#8220;-(1-2)&#8221